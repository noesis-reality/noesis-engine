// automatically generated by the FlatBuffers compiler, do not modify

package noesis.protocol

import com.google.flatbuffers.kotlin.*
import kotlin.jvm.JvmInline
@Suppress("unused")
class InferenceRequest : Table() {

    fun init(i: Int, buffer: ReadWriteBuffer) : InferenceRequest = reset(i, buffer)

    val id : String? get() = lookupField(4, null ) { string(it + bufferPos) }
    fun idAsBuffer() : ReadBuffer = vectorAsBuffer(bb, 4, 1)

    val prompt : String? get() = lookupField(6, null ) { string(it + bufferPos) }
    fun promptAsBuffer() : ReadBuffer = vectorAsBuffer(bb, 6, 1)

    val systemPrompt : String? get() = lookupField(8, null ) { string(it + bufferPos) }
    fun systemPromptAsBuffer() : ReadBuffer = vectorAsBuffer(bb, 8, 1)

    val maxTokens : UInt get() = lookupField(10, 100u ) { bb.getUInt(it + bufferPos) }

    val temperature : Float get() = lookupField(12, 0.7f ) { bb.getFloat(it + bufferPos) }

    val topP : Float get() = lookupField(14, 0.9f ) { bb.getFloat(it + bufferPos) }

    val repetitionPenalty : Float get() = lookupField(16, 1.1f ) { bb.getFloat(it + bufferPos) }

    val reasoningEffort : noesis.protocol.ReasoningLevel get() = lookupField(18, noesis.protocol.ReasoningLevel(1) ) { noesis.protocol.ReasoningLevel(bb.get(it + bufferPos)) }

    val seed : UInt get() = lookupField(20, 0u ) { bb.getUInt(it + bufferPos) }

    val streamingMode : noesis.protocol.StreamingMode get() = lookupField(22, noesis.protocol.StreamingMode(0) ) { noesis.protocol.StreamingMode(bb.get(it + bufferPos)) }

    val streamingBatchSize : UInt get() = lookupField(24, 1u ) { bb.getUInt(it + bufferPos) }

    val priority : Byte get() = lookupField(26, 0 ) { bb.get(it + bufferPos) }

    val timeoutMs : UInt get() = lookupField(28, 30000u ) { bb.getUInt(it + bufferPos) }

    companion object {
        fun validateVersion() = VERSION_2_0_8

        fun asRoot(buffer: ReadWriteBuffer) : InferenceRequest = asRoot(buffer, InferenceRequest())
        fun asRoot(buffer: ReadWriteBuffer, obj: InferenceRequest) : InferenceRequest = obj.init(buffer.getInt(buffer.limit) + buffer.limit, buffer)


        fun createInferenceRequest(builder: FlatBufferBuilder, idOffset: Offset<String>, promptOffset: Offset<String>, systemPromptOffset: Offset<String>, maxTokens: UInt, temperature: Float, topP: Float, repetitionPenalty: Float, reasoningEffort: noesis.protocol.ReasoningLevel, seed: UInt, streamingMode: noesis.protocol.StreamingMode, streamingBatchSize: UInt, priority: Byte, timeoutMs: UInt) : Offset<InferenceRequest> {
            builder.startTable(13)
            addTimeoutMs(builder, timeoutMs)
            addStreamingBatchSize(builder, streamingBatchSize)
            addSeed(builder, seed)
            addRepetitionPenalty(builder, repetitionPenalty)
            addTopP(builder, topP)
            addTemperature(builder, temperature)
            addMaxTokens(builder, maxTokens)
            addSystemPrompt(builder, systemPromptOffset)
            addPrompt(builder, promptOffset)
            addId(builder, idOffset)
            addPriority(builder, priority)
            addStreamingMode(builder, streamingMode)
            addReasoningEffort(builder, reasoningEffort)
            return endInferenceRequest(builder)
        }
        fun startInferenceRequest(builder: FlatBufferBuilder) = builder.startTable(13)

        fun addId(builder: FlatBufferBuilder, id: Offset<String>) = builder.add(0, id, 0)

        fun addPrompt(builder: FlatBufferBuilder, prompt: Offset<String>) = builder.add(1, prompt, 0)

        fun addSystemPrompt(builder: FlatBufferBuilder, systemPrompt: Offset<String>) = builder.add(2, systemPrompt, 0)

        fun addMaxTokens(builder: FlatBufferBuilder, maxTokens: UInt) = builder.add(3, maxTokens, 100u)

        fun addTemperature(builder: FlatBufferBuilder, temperature: Float) = builder.add(4, temperature, 0.7f)

        fun addTopP(builder: FlatBufferBuilder, topP: Float) = builder.add(5, topP, 0.9f)

        fun addRepetitionPenalty(builder: FlatBufferBuilder, repetitionPenalty: Float) = builder.add(6, repetitionPenalty, 1.1f)

        fun addReasoningEffort(builder: FlatBufferBuilder, reasoningEffort: noesis.protocol.ReasoningLevel) = builder.add(7, reasoningEffort.value, 1)

        fun addSeed(builder: FlatBufferBuilder, seed: UInt) = builder.add(8, seed, 0u)

        fun addStreamingMode(builder: FlatBufferBuilder, streamingMode: noesis.protocol.StreamingMode) = builder.add(9, streamingMode.value, 0)

        fun addStreamingBatchSize(builder: FlatBufferBuilder, streamingBatchSize: UInt) = builder.add(10, streamingBatchSize, 1u)

        fun addPriority(builder: FlatBufferBuilder, priority: Byte) = builder.add(11, priority, 0)

        fun addTimeoutMs(builder: FlatBufferBuilder, timeoutMs: UInt) = builder.add(12, timeoutMs, 30000u)

        fun endInferenceRequest(builder: FlatBufferBuilder) : Offset<InferenceRequest> {
            val o: Offset<InferenceRequest> = builder.endTable()
            return o
        }
    }
}

typealias InferenceRequestOffsetArray = OffsetArray<InferenceRequest>

inline fun InferenceRequestOffsetArray(size: Int, crossinline call: (Int) -> Offset<InferenceRequest>): InferenceRequestOffsetArray =
    InferenceRequestOffsetArray(IntArray(size) { call(it).value })
